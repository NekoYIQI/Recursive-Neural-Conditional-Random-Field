{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type :  <class 'pandas.core.frame.DataFrame'>\n",
      "Data dims :  6685900\n"
     ]
    }
   ],
   "source": [
    "print(\"Data type : \", type(text))\n",
    "print(\"Data dims : \", text.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert csv file to txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"laptop_review.txt\", \"w\") as my_output_file:\n",
    "    with open(\"reviews_Electronics_5.csv\", \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NLTK tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"Total bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge us $69 for 3 pills. I checked online the pills can be had for 19 cents EACH! Avoid Hospital ERs at all costs. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert txt file to list of lists of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('review.txt') as fin:\n",
    "    content = fin.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Total bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge us $69 for 3 pills. I checked online the pills can be had for 19 cents EACH! Avoid Hospital ERs at all costs.\\n', \"I *adore* Travis at the Hard Rock's new Kelly Cardenas Salon!  I'm always a fan of a great blowout and no stranger to the chains that offer this service; however, Travis has taken the flawless blowout to a whole new level!  \\n\", '\\n', \"Travis's greets you with his perfectly green swoosh in his otherwise perfectly styled black hair and a Vegas-worthy rockstar outfit.  Next comes the most relaxing and incredible shampoo -- where you get a full head message that could cure even the very worst migraine in minutes --- and the scented shampoo room.  Travis has freakishly strong fingers (in a good way) and use the perfect amount of pressure.  That was superb!  Then starts the glorious blowout... where not one, not two, but THREE people were involved in doing the best round-brush action my hair has ever seen.  The team of stylists clearly gets along extremely well, as it's evident from the way they talk to and help one another that it's really genuine and not some corporate requirement.  It was so much fun to be there! \\n\", '\\n', \"Next Travis started with the flat iron.  The way he flipped his wrist to get volume all around without over-doing it and making me look like a Texas pagent girl was admirable.  It's also worth noting that he didn't fry my hair -- something that I've had happen before with less skilled stylists.  At the end of the blowout & style my hair was perfectly bouncey and looked terrific.  The only thing better?  That this awesome blowout lasted for days! \\n\", '\\n', \"Travis, I will see you every single time I'm out in Vegas.  You make me feel beauuuutiful!\\n\", \"I have to say that this office really has it together, they are so organized and friendly!  Dr. J. Phillipp is a great dentist, very friendly and professional.  The dental assistants that helped in my procedure were amazing, Jewel and Bailey helped me to feel comfortable!  I don't have dental insurance, but they have this insurance through their office you can purchase for $80 something a year and this gave me 25% off all of my dental work, plus they helped me get signed up for care credit which I knew nothing about before this visit!  I highly recommend this office for the nice synergy the whole office has!\\n\", \"Went in for a lunch. Steak sandwich was delicious, and the Caesar salad had an absolutely delicious dressing, with a perfect amount of dressing, and distributed perfectly across each leaf. I know I'm going on about the salad ... But it was perfect.\\n\"]\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "content = content[1:1000001]\n",
    "print(content[:10])\n",
    "print(len(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess:\n",
    "### 1. Remove newline characters from each sentence  \n",
    "### 2. Avoid tokenize empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [x.strip() for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "print(len(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use first 10 sentences for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "[['Total', 'bill', 'for', 'this', 'horrible', 'service', '?', 'Over', '$', '8Gs', '.', 'These', 'crooks', 'actually', 'had', 'the', 'nerve', 'to', 'charge', 'us', '$', '69', 'for', '3', 'pills', '.', 'I', 'checked', 'online', 'the', 'pills', 'can', 'be', 'had', 'for', '19', 'cents', 'EACH', '!', 'Avoid', 'Hospital', 'ERs', 'at', 'all', 'costs', '.'], ['I', '*adore*', 'Travis', 'at', 'the', 'Hard', 'Rock', \"'s\", 'new', 'Kelly', 'Cardenas', 'Salon', '!', 'I', \"'m\", 'always', 'a', 'fan', 'of', 'a', 'great', 'blowout', 'and', 'no', 'stranger', 'to', 'the', 'chains', 'that', 'offer', 'this', 'service', ';', 'however', ',', 'Travis', 'has', 'taken', 'the', 'flawless', 'blowout', 'to', 'a', 'whole', 'new', 'level', '!'], ['Travis', \"'s\", 'greets', 'you', 'with', 'his', 'perfectly', 'green', 'swoosh', 'in', 'his', 'otherwise', 'perfectly', 'styled', 'black', 'hair', 'and', 'a', 'Vegas-worthy', 'rockstar', 'outfit', '.', 'Next', 'comes', 'the', 'most', 'relaxing', 'and', 'incredible', 'shampoo', '--', 'where', 'you', 'get', 'a', 'full', 'head', 'message', 'that', 'could', 'cure', 'even', 'the', 'very', 'worst', 'migraine', 'in', 'minutes', '--', '-', 'and', 'the', 'scented', 'shampoo', 'room', '.', 'Travis', 'has', 'freakishly', 'strong', 'fingers', '(', 'in', 'a', 'good', 'way', ')', 'and', 'use', 'the', 'perfect', 'amount', 'of', 'pressure', '.', 'That', 'was', 'superb', '!', 'Then', 'starts', 'the', 'glorious', 'blowout', '...', 'where', 'not', 'one', ',', 'not', 'two', ',', 'but', 'THREE', 'people', 'were', 'involved', 'in', 'doing', 'the', 'best', 'round-brush', 'action', 'my', 'hair', 'has', 'ever', 'seen', '.', 'The', 'team', 'of', 'stylists', 'clearly', 'gets', 'along', 'extremely', 'well', ',', 'as', 'it', \"'s\", 'evident', 'from', 'the', 'way', 'they', 'talk', 'to', 'and', 'help', 'one', 'another', 'that', 'it', \"'s\", 'really', 'genuine', 'and', 'not', 'some', 'corporate', 'requirement', '.', 'It', 'was', 'so', 'much', 'fun', 'to', 'be', 'there', '!'], ['Next', 'Travis', 'started', 'with', 'the', 'flat', 'iron', '.', 'The', 'way', 'he', 'flipped', 'his', 'wrist', 'to', 'get', 'volume', 'all', 'around', 'without', 'over-doing', 'it', 'and', 'making', 'me', 'look', 'like', 'a', 'Texas', 'pagent', 'girl', 'was', 'admirable', '.', 'It', \"'s\", 'also', 'worth', 'noting', 'that', 'he', 'did', \"n't\", 'fry', 'my', 'hair', '--', 'something', 'that', 'I', \"'ve\", 'had', 'happen', 'before', 'with', 'less', 'skilled', 'stylists', '.', 'At', 'the', 'end', 'of', 'the', 'blowout', '&', 'style', 'my', 'hair', 'was', 'perfectly', 'bouncey', 'and', 'looked', 'terrific', '.', 'The', 'only', 'thing', 'better', '?', 'That', 'this', 'awesome', 'blowout', 'lasted', 'for', 'days', '!'], ['Travis', ',', 'I', 'will', 'see', 'you', 'every', 'single', 'time', 'I', \"'m\", 'out', 'in', 'Vegas', '.', 'You', 'make', 'me', 'feel', 'beauuuutiful', '!'], ['I', 'have', 'to', 'say', 'that', 'this', 'office', 'really', 'has', 'it', 'together', ',', 'they', 'are', 'so', 'organized', 'and', 'friendly', '!', 'Dr.', 'J.', 'Phillipp', 'is', 'a', 'great', 'dentist', ',', 'very', 'friendly', 'and', 'professional', '.', 'The', 'dental', 'assistants', 'that', 'helped', 'in', 'my', 'procedure', 'were', 'amazing', ',', 'Jewel', 'and', 'Bailey', 'helped', 'me', 'to', 'feel', 'comfortable', '!', 'I', 'do', \"n't\", 'have', 'dental', 'insurance', ',', 'but', 'they', 'have', 'this', 'insurance', 'through', 'their', 'office', 'you', 'can', 'purchase', 'for', '$', '80', 'something', 'a', 'year', 'and', 'this', 'gave', 'me', '25', '%', 'off', 'all', 'of', 'my', 'dental', 'work', ',', 'plus', 'they', 'helped', 'me', 'get', 'signed', 'up', 'for', 'care', 'credit', 'which', 'I', 'knew', 'nothing', 'about', 'before', 'this', 'visit', '!', 'I', 'highly', 'recommend', 'this', 'office', 'for', 'the', 'nice', 'synergy', 'the', 'whole', 'office', 'has', '!'], ['Went', 'in', 'for', 'a', 'lunch', '.', 'Steak', 'sandwich', 'was', 'delicious', ',', 'and', 'the', 'Caesar', 'salad', 'had', 'an', 'absolutely', 'delicious', 'dressing', ',', 'with', 'a', 'perfect', 'amount', 'of', 'dressing', ',', 'and', 'distributed', 'perfectly', 'across', 'each', 'leaf', '.', 'I', 'know', 'I', \"'m\", 'going', 'on', 'about', 'the', 'salad', '...', 'But', 'it', 'was', 'perfect', '.'], ['Drink', 'prices', 'were', 'pretty', 'good', '.'], ['The', 'Server', ',', 'Dawn', ',', 'was', 'friendly', 'and', 'accommodating', '.', 'Very', 'happy', 'with', 'her', '.'], ['In', 'summation', ',', 'a', 'great', 'pub', 'experience', '.', 'Would', 'go', 'again', '!']]\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "i = 0\n",
    "\n",
    "for item in content:\n",
    "    if item != '':\n",
    "        token = nltk.word_tokenize(item)\n",
    "        if i%100000 == 0:\n",
    "            print(i)\n",
    "        if i == 500000:\n",
    "            break\n",
    "        i = i+1\n",
    "        tokens.append(token)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read and process txt file by file to avoid memory overloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "fin.close()\n",
    "print(fin.closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "fout = open('word2vec.txt', 'wb+')\n",
    "model = Word2Vec(tokens, size=300, min_count=1)\n",
    "model.wv.save_word2vec_format(fout, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert \",\" after each word in the word2vec file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". -2.0189521 -5.8182044 0.14730741 -2.4438872 -2.3432002 0.027203854 0.56074858 1.9747018 -0.9619664 -2.2359681 0.7151807 1.8614836 -1.073871 1.0667757 0.26452845 -4.4457617 -2.0084248 1.009846 0.66925508 -0.079265475 2.0818181 0.70153391 1.6127125 -1.8699176 -3.6519375 3.973099 -1.6392468 -1.7572402 -0.14976385 5.3115864 -0.44981936 1.6329498 0.52399665 0.0039508515 -2.1583259 1.4142026 -3.4555173 -2.6381121 -2.0756478 -2.5409286 -1.622468 1.1533537 -0.2284846 0.87708282 0.38823301 3.5241346 0.68905777 -0.90394467 -2.6581752 -0.24261177 -1.5700191 2.8633161 -1.292263 -1.4047838 -0.61583155 -0.49841624 1.8368499 2.3685167 1.624339 -1.7189536 1.0612468 5.4982815 -0.78282255 -1.3522865 -1.8656341 0.63540995 1.6501132 0.76236647 0.77221495 -3.9317036 -2.2047625 1.5622901 1.5328445 3.5679202 1.1807374 -1.7676226 -0.70877355 3.6269639 -0.45001313 -3.1530521 3.2267063 -1.9233863 -0.29651436 0.28170928 3.5603006 -2.3688693 0.49584836 1.5836004 2.5265429 0.47179022 0.64063007 0.78018373 -0.82015032 -4.4724522 2.2339294 -0.7031514 -5.0736189 -2.1756837 2.1705909 3.1267779\n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "with open('word2vec.txt') as fin:\n",
    "    for line in fin:\n",
    "        if i > 0:\n",
    "            print(line)\n",
    "            print(type(line))\n",
    "            break;\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test word, vector processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace whitespace following the word with comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".,-2.0189521,-5.8182044,0.14730741,-2.4438872,-2.3432002,0.027203854,0.56074858,1.9747018,-0.9619664,-2.2359681,0.7151807,1.8614836,-1.073871,1.0667757,0.26452845,-4.4457617,-2.0084248,1.009846,0.66925508,-0.079265475,2.0818181,0.70153391,1.6127125,-1.8699176,-3.6519375,3.973099,-1.6392468,-1.7572402,-0.14976385,5.3115864,-0.44981936,1.6329498,0.52399665,0.0039508515,-2.1583259,1.4142026,-3.4555173,-2.6381121,-2.0756478,-2.5409286,-1.622468,1.1533537,-0.2284846,0.87708282,0.38823301,3.5241346,0.68905777,-0.90394467,-2.6581752,-0.24261177,-1.5700191,2.8633161,-1.292263,-1.4047838,-0.61583155,-0.49841624,1.8368499,2.3685167,1.624339,-1.7189536,1.0612468,5.4982815,-0.78282255,-1.3522865,-1.8656341,0.63540995,1.6501132,0.76236647,0.77221495,-3.9317036,-2.2047625,1.5622901,1.5328445,3.5679202,1.1807374,-1.7676226,-0.70877355,3.6269639,-0.45001313,-3.1530521,3.2267063,-1.9233863,-0.29651436,0.28170928,3.5603006,-2.3688693,0.49584836,1.5836004,2.5265429,0.47179022,0.64063007,0.78018373,-0.82015032,-4.4724522,2.2339294,-0.7031514,-5.0736189,-2.1756837,2.1705909,3.1267779\n"
     ]
    }
   ],
   "source": [
    "str = \". -2.0189521 -5.8182044 0.14730741 -2.4438872 -2.3432002 0.027203854 0.56074858 1.9747018 -0.9619664 -2.2359681 0.7151807 1.8614836 -1.073871 1.0667757 0.26452845 -4.4457617 -2.0084248 1.009846 0.66925508 -0.079265475 2.0818181 0.70153391 1.6127125 -1.8699176 -3.6519375 3.973099 -1.6392468 -1.7572402 -0.14976385 5.3115864 -0.44981936 1.6329498 0.52399665 0.0039508515 -2.1583259 1.4142026 -3.4555173 -2.6381121 -2.0756478 -2.5409286 -1.622468 1.1533537 -0.2284846 0.87708282 0.38823301 3.5241346 0.68905777 -0.90394467 -2.6581752 -0.24261177 -1.5700191 2.8633161 -1.292263 -1.4047838 -0.61583155 -0.49841624 1.8368499 2.3685167 1.624339 -1.7189536 1.0612468 5.4982815 -0.78282255 -1.3522865 -1.8656341 0.63540995 1.6501132 0.76236647 0.77221495 -3.9317036 -2.2047625 1.5622901 1.5328445 3.5679202 1.1807374 -1.7676226 -0.70877355 3.6269639 -0.45001313 -3.1530521 3.2267063 -1.9233863 -0.29651436 0.28170928 3.5603006 -2.3688693 0.49584836 1.5836004 2.5265429 0.47179022 0.64063007 0.78018373 -0.82015032 -4.4724522 2.2339294 -0.7031514 -5.0736189 -2.1756837 2.1705909 3.1267779\"\n",
    "str = str.replace(' ', ',')\n",
    "print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "['-2.0189521' '-5.8182044' '0.14730741' '-2.4438872' '-2.3432002'\n",
      " '0.027203854' '0.56074858' '1.9747018' '-0.9619664' '-2.2359681'\n",
      " '0.7151807' '1.8614836' '-1.073871' '1.0667757' '0.26452845' '-4.4457617'\n",
      " '-2.0084248' '1.009846' '0.66925508' '-0.079265475' '2.0818181'\n",
      " '0.70153391' '1.6127125' '-1.8699176' '-3.6519375' '3.973099' '-1.6392468'\n",
      " '-1.7572402' '-0.14976385' '5.3115864' '-0.44981936' '1.6329498'\n",
      " '0.52399665' '0.0039508515' '-2.1583259' '1.4142026' '-3.4555173'\n",
      " '-2.6381121' '-2.0756478' '-2.5409286' '-1.622468' '1.1533537'\n",
      " '-0.2284846' '0.87708282' '0.38823301' '3.5241346' '0.68905777'\n",
      " '-0.90394467' '-2.6581752' '-0.24261177' '-1.5700191' '2.8633161'\n",
      " '-1.292263' '-1.4047838' '-0.61583155' '-0.49841624' '1.8368499'\n",
      " '2.3685167' '1.624339' '-1.7189536' '1.0612468' '5.4982815' '-0.78282255'\n",
      " '-1.3522865' '-1.8656341' '0.63540995' '1.6501132' '0.76236647'\n",
      " '0.77221495' '-3.9317036' '-2.2047625' '1.5622901' '1.5328445' '3.5679202'\n",
      " '1.1807374' '-1.7676226' '-0.70877355' '3.6269639' '-0.45001313'\n",
      " '-3.1530521' '3.2267063' '-1.9233863' '-0.29651436' '0.28170928'\n",
      " '3.5603006' '-2.3688693' '0.49584836' '1.5836004' '2.5265429' '0.47179022'\n",
      " '0.64063007' '0.78018373' '-0.82015032' '-4.4724522' '2.2339294'\n",
      " '-0.7031514' '-5.0736189' '-2.1756837' '2.1705909']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "word_vector = str.split(\",\")\n",
    "i = 1\n",
    "while '.' not in word_vector[i] or '..' in word_vector[i] or word_vector[i] == '.':\n",
    "    i += 1\n",
    "        \n",
    "word = ','.join(word_vector[:i])\n",
    "    \n",
    "vector_list = []\n",
    "for element in word_vector[i:len(word_vector)-1]:\n",
    "    vector_list.append(element)\n",
    "        \n",
    "vector = np.asarray(vector_list)\n",
    "#     dictionary[word] = vector\n",
    "print(word)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = open('word2vec_final.txt', 'w')\n",
    "with open('word2vec.txt') as fin:\n",
    "    for line in fin:\n",
    "        line = line.replace(' ', ',')\n",
    "        fout.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with more memory-friendly way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word2vec.LineSentence(\"laptop_review.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences,size=300, window=3, min_count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = open('word2vec_laptop.txt', 'wb')\n",
    "model.wv.save_word2vec_format(fout, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_file = open('word2vec.txt', 'r')\n",
    "word2vec = word2vec_file.readlines()\n",
    "word2vec_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674286\n"
     ]
    }
   ],
   "source": [
    "print(len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "492ae2f8-09cb-4b49-af6f-204e74346517",
    "theme": {
     "492ae2f8-09cb-4b49-af6f-204e74346517": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "492ae2f8-09cb-4b49-af6f-204e74346517",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         34,
         34,
         34
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         256,
         256,
         256
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         66,
         175,
         250
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         256,
         256,
         256
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 5.25
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 4
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3.5
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       },
       "p": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Source Sans Pro",
       "font-size": 6
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
